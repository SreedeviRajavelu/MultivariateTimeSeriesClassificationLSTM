{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9819595,"sourceType":"datasetVersion","datasetId":6020780}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T15:59:10.693153Z","iopub.execute_input":"2024-11-09T15:59:10.693638Z","iopub.status.idle":"2024-11-09T15:59:10.876186Z","shell.execute_reply.started":"2024-11-09T15:59:10.693591Z","shell.execute_reply":"2024-11-09T15:59:10.875020Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/readme.txt\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/camera.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2nd column is valence, 3rd column is arousal \n\n# 31 rows x 4 columns","metadata":{}},{"cell_type":"code","source":"sample_arousal_valence = pd.read_csv(\"/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\")\nprint(sample_arousal_valence)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.878248Z","iopub.execute_input":"2024-11-09T15:59:10.878663Z","iopub.status.idle":"2024-11-09T15:59:10.889584Z","shell.execute_reply.started":"2024-11-09T15:59:10.878623Z","shell.execute_reply":"2024-11-09T15:59:10.888407Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"    16  6  3  4\n0   17  3  5  6\n1   18  5  5  7\n2   19  6  7  6\n3   20  6  5  6\n4   21  7  4  5\n5   22  5  2  3\n6   23  5  3  5\n7    8  5  5  7\n8    9  7  2  5\n9   10  6  4  5\n10  11  7  4  5\n11  12  6  5  7\n12  13  8  3  6\n13  14  5  3  6\n14  15  7  2  6\n15  24  5  5  6\n16  25  6  7  8\n17  26  3  5  7\n18  27  6  6  7\n19  28  5  6  7\n20  29  5  6  5\n21  30  6  5  6\n22  31  6  7  5\n23   0  4  7  7\n24   1  6  6  6\n25   2  5  6  8\n26   3  3  5  9\n27   4  5  6  6\n28   5  3  5  9\n29   6  5  5  6\n30   7  5  7  7\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_arousal_valence.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.890949Z","iopub.execute_input":"2024-11-09T15:59:10.891325Z","iopub.status.idle":"2024-11-09T15:59:10.901786Z","shell.execute_reply.started":"2024-11-09T15:59:10.891287Z","shell.execute_reply":"2024-11-09T15:59:10.900553Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(31, 4)"},"metadata":{}}]},{"cell_type":"code","source":"# import pandas as pd\n\n# sample_raw_gsr = pd.read_csv('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', delimiter=',', encoding='latin1')\n# # print(sample_raw_ppg)\n\nwith open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for _ in range(5):\n        print(file.readline())\n        \n# sample_raw_gsr = pd.read_csv('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.904538Z","iopub.execute_input":"2024-11-09T15:59:10.904923Z","iopub.status.idle":"2024-11-09T15:59:10.921320Z","shell.execute_reply.started":"2024-11-09T15:59:10.904885Z","shell.execute_reply":"2024-11-09T15:59:10.920162Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"1653098503514,167626.65625\n\n,167471.5\n\n,167102.015625\n\n,166955.375\n\n1653098504514,166100.59375\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for i, line in enumerate(file):\n        if 40 <= i <= 50:  # Print lines near the problematic line\n            print(f\"Line {i}: {line}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.922687Z","iopub.execute_input":"2024-11-09T15:59:10.923095Z","iopub.status.idle":"2024-11-09T15:59:10.937226Z","shell.execute_reply.started":"2024-11-09T15:59:10.923042Z","shell.execute_reply":"2024-11-09T15:59:10.935929Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Line 40: 1653098513514,160582.453125\n\nLine 41: ,161048.28125\n\nLine 42: ,161530.78125\n\nLine 43: ,161736.203125\n\nLine 44: 1653098514514,162662.21875,60\n\nLine 45: ,162934.453125\n\nLine 46: ,163200.359375\n\nLine 47: ,162932.984375\n\nLine 48: 1653098515514,161923.4375\n\nLine 49: ,160348.984375\n\nLine 50: ,158409.1875\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for _ in range(55):\n        print(file.readline())","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.938585Z","iopub.execute_input":"2024-11-09T15:59:10.938942Z","iopub.status.idle":"2024-11-09T15:59:10.946364Z","shell.execute_reply.started":"2024-11-09T15:59:10.938905Z","shell.execute_reply":"2024-11-09T15:59:10.945299Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"1653098503514,167626.65625\n\n,167471.5\n\n,167102.015625\n\n,166955.375\n\n1653098504514,166100.59375\n\n,166128.859375\n\n,165834.359375\n\n,164697.875\n\n1653098505514,162906.546875\n\n,160625.859375\n\n,157993.640625\n\n,155868.5625\n\n1653098506514,154536.578125\n\n,153685.015625\n\n,153312.03125\n\n,153390.703125\n\n1653098507514,153859.90625\n\n,154674.765625\n\n,155562.28125\n\n,156477.078125\n\n1653098508514,157111.984375\n\n,157485.46875\n\n,157590.515625\n\n,157376.78125\n\n1653098509514,156990.3125\n\n,156641.3125\n\n,156430.09375\n\n,156397.296875\n\n1653098510514,156580.859375\n\n,158044.28125\n\n,157763.8125\n\n,157835.984375\n\n1653098511514,158431.25\n\n,159343.4375\n\n,159119.765625\n\n,160388.265625\n\n1653098512514,160691.28125\n\n,160694.125\n\n,160889.1875\n\n,160626.421875\n\n1653098513514,160582.453125\n\n,161048.28125\n\n,161530.78125\n\n,161736.203125\n\n1653098514514,162662.21875,60\n\n,162934.453125\n\n,163200.359375\n\n,162932.984375\n\n1653098515514,161923.4375\n\n,160348.984375\n\n,158409.1875\n\n,156600.25\n\n1653098516514,155188.796875\n\n,154266.671875\n\n,153796.03125\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# for gsr\n\nsample_raw_gsr = pd.read_csv(\n    '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', \n    names=['timestamp', 'gsr_value', 'extra_column'],  # Define expected columns\n    na_values=[''],  # Treat empty strings as NaN\n    skiprows=1  # Skip header if needed\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.947979Z","iopub.execute_input":"2024-11-09T15:59:10.948361Z","iopub.status.idle":"2024-11-09T15:59:10.966070Z","shell.execute_reply.started":"2024-11-09T15:59:10.948324Z","shell.execute_reply":"2024-11-09T15:59:10.964904Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(sample_raw_gsr)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.967471Z","iopub.execute_input":"2024-11-09T15:59:10.967959Z","iopub.status.idle":"2024-11-09T15:59:10.979427Z","shell.execute_reply.started":"2024-11-09T15:59:10.967904Z","shell.execute_reply":"2024-11-09T15:59:10.977990Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"          timestamp      gsr_value  extra_column\n0               NaN  167471.500000           NaN\n1               NaN  167102.015625           NaN\n2               NaN  166955.375000           NaN\n3      1.653099e+12  166100.593750           NaN\n4               NaN  166128.859375           NaN\n...             ...            ...           ...\n14682           NaN  158623.375000           NaN\n14683  1.653102e+12  158737.984375           NaN\n14684           NaN  159241.437500           NaN\n14685           NaN  159396.109375           NaN\n14686           NaN  159654.781250           NaN\n\n[14687 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(sample_raw_gsr['gsr_value'])\n\ngsr_10 = sample_raw_gsr['gsr_value']\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.980965Z","iopub.execute_input":"2024-11-09T15:59:10.981333Z","iopub.status.idle":"2024-11-09T15:59:10.991473Z","shell.execute_reply.started":"2024-11-09T15:59:10.981292Z","shell.execute_reply":"2024-11-09T15:59:10.990391Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"0        167471.500000\n1        167102.015625\n2        166955.375000\n3        166100.593750\n4        166128.859375\n             ...      \n14682    158623.375000\n14683    158737.984375\n14684    159241.437500\n14685    159396.109375\n14686    159654.781250\nName: gsr_value, Length: 14687, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(gsr_10)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:10.995326Z","iopub.execute_input":"2024-11-09T15:59:10.995860Z","iopub.status.idle":"2024-11-09T15:59:11.004593Z","shell.execute_reply.started":"2024-11-09T15:59:10.995813Z","shell.execute_reply":"2024-11-09T15:59:11.003443Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"0        167471.500000\n1        167102.015625\n2        166955.375000\n3        166100.593750\n4        166128.859375\n             ...      \n14682    158623.375000\n14683    158737.984375\n14684    159241.437500\n14685    159396.109375\n14686    159654.781250\nName: gsr_value, Length: 14687, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"gsr_10.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:11.005885Z","iopub.execute_input":"2024-11-09T15:59:11.006241Z","iopub.status.idle":"2024-11-09T15:59:11.021986Z","shell.execute_reply.started":"2024-11-09T15:59:11.006203Z","shell.execute_reply":"2024-11-09T15:59:11.020527Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(14687,)"},"metadata":{}}]},{"cell_type":"code","source":"# for ppg\n\nsample_raw_ppg = pd.read_csv(\n   '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_ppg.csv', \n    names=['timestamp', 'ppg_value', 'extra_column'],  # Define expected columns\n    na_values=[''],  # Treat empty strings as NaN\n    skiprows=1  # Skip header if needed\n)\n\nprint(sample_raw_ppg['ppg_value'])\nppg_10 = sample_raw_ppg['ppg_value']","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:11.023834Z","iopub.execute_input":"2024-11-09T15:59:11.024252Z","iopub.status.idle":"2024-11-09T15:59:11.173245Z","shell.execute_reply.started":"2024-11-09T15:59:11.024211Z","shell.execute_reply":"2024-11-09T15:59:11.172197Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"0         956083.0\n1         957633.0\n2         959069.0\n3         960354.0\n4         961414.0\n            ...   \n348594    520632.0\n348595    520862.0\n348596    521081.0\n348597    521116.0\n348598    521097.0\nName: ppg_value, Length: 348599, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"ppg_10.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:11.176146Z","iopub.execute_input":"2024-11-09T15:59:11.176510Z","iopub.status.idle":"2024-11-09T15:59:11.182959Z","shell.execute_reply.started":"2024-11-09T15:59:11.176457Z","shell.execute_reply":"2024-11-09T15:59:11.181983Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(348599,)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Organize data as shown below :\n\nX_train :\n\n| subject_id | sequence_id | ppg_series | gsr_series |\n|------------|-------------|------------|------------|\n| 1          | 1           | [...]      | [...]      |\n| 1          | 2           | [...]      | [...]      |\n| 1          | 3           | [...]      | [...]      |\n| 2          | 1           | [...]      | [...]      |\n| 2          | 2           | [...]      | [...]      |\n| 2          | 3           | [...]      | [...]      |\n\n\ny_train :\n\n| subject_id | sequence_id | arousal | valence |\n|------------|-------------|---------|---------|\n| 1          | 1           | 7       | 6       |\n| 1          | 2           | 9       | 8       |\n| 1          | 3           | 4       | 5       |\n| 2          | 1           | 6       | 2       |\n| 2          | 2           | 8       | 4       |\n| 2          | 3           | 5       | 6       |","metadata":{}},{"cell_type":"markdown","source":"# Updated features_target dataframe\n\n| subject_id | series_id | ppg_series | gsr_series | Target |\n|------------|-----------|------------|------------|--------|\n| 1          | 1         | [...]      | [...]      | AxVy   |\n| 1          | 2         | [...]      | [...]      | AxVy   |\n| 1          | 3         | [...]      | [...]      | AxVy   |\n| 2          | 1         | [...]      | [...]      | AxVy   |\n| 2          | 2         | [...]      | [...]      | AxVy   |\n| 2          | 3         | [...]      | [...]      | AxVy   |\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\n# Initialize an empty list to store data for each row\ndata = []\n\n# Define the base directory\nbase_dir = '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/'\n\n# Loop through each subject's subdirectory\nfor subject_folder in os.listdir(base_dir):\n    subject_dir = os.path.join(base_dir, subject_folder, subject_folder)\n    \n    if os.path.isdir(subject_dir):  # Check if it’s a directory\n        subject_id = int(subject_folder)  # Extract subject_id from folder name\n        \n        # Load raw_gsr.csv and raw_ppg.csv with specific column names\n        raw_gsr_path = os.path.join(subject_dir, 'raw_gsr.csv')\n        raw_ppg_path = os.path.join(subject_dir, 'raw_ppg.csv')\n        \n        # Define column names and load the data, treating empty strings as NaN and skipping the header row\n        raw_gsr = pd.read_csv(\n            raw_gsr_path, \n            names=['timestamp', 'gsr_value', 'extra_column'], \n            na_values=[''], \n            skiprows=1\n        )['gsr_value']  # Select only the 'gsr_value' column\n\n        raw_ppg = pd.read_csv(\n            raw_ppg_path, \n            names=['timestamp', 'ppg_value', 'extra_column'], \n            na_values=[''], \n            skiprows=1\n        )['ppg_value']  # Select only the 'ppg_value' column\n        \n        # Calculate downsampling factor\n        gsr_downsample_factor = len(raw_gsr) // 6000\n        ppg_downsample_factor = len(raw_ppg) // 6000\n        \n        # Downsample by taking every nth data point\n        downsampled_gsr = raw_gsr.iloc[::gsr_downsample_factor].head(6000)\n        downsampled_ppg = raw_ppg.iloc[::ppg_downsample_factor].head(6000)\n        \n        # Split the downsampled data into chunks of 200 for each row in the DataFrame\n        gsr_chunks = np.array_split(downsampled_gsr.values.flatten(), 30)\n        ppg_chunks = np.array_split(downsampled_ppg.values.flatten(), 30)\n        \n        # Load Arousal_Valence.csv for target labels\n        av_path = os.path.join(subject_dir, 'Arousal_Valence.csv')\n        av_data = pd.read_csv(av_path)\n\n        for series_id in range(1, 31):  # For each of the 30 sequences per subject\n            # Extract the chunk of 200 values for gsr_series and ppg_series\n            gsr_series = gsr_chunks[series_id - 1]\n            ppg_series = ppg_chunks[series_id - 1]\n            \n            # Create target label in \"AxVy\" format\n            valence = int(av_data.iloc[series_id - 1, 1])  # Second column is Valence\n            arousal = int(av_data.iloc[series_id - 1, 2])  # Third column is Arousal\n            target = f\"A{arousal}V{valence}\"\n            \n            # Append the data to the list\n            data.append({\n                'subject_id': subject_id,\n                'series_id': series_id,\n                'ppg_series': ppg_series,\n                'gsr_series': gsr_series,\n                'target': target\n            })\n\n# Convert the list of dictionaries to a DataFrame\nfeatures_target = pd.DataFrame(data)\n\n# Display the first few rows of the DataFrame\nfeatures_target.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:11.184706Z","iopub.execute_input":"2024-11-09T15:59:11.185074Z","iopub.status.idle":"2024-11-09T15:59:21.358373Z","shell.execute_reply.started":"2024-11-09T15:59:11.185036Z","shell.execute_reply":"2024-11-09T15:59:21.357181Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"   subject_id  series_id                                         ppg_series  \\\n0           7          1  [799777.0, 800031.0, 770530.0, 783136.0, 77184...   \n1           7          2  [782666.0, 781125.0, 786560.0, 793689.0, 79145...   \n2           7          3  [783222.0, 774507.0, 775848.0, 780017.0, 77640...   \n3           7          4  [631551.0, 644157.0, 663125.0, 658480.0, 65069...   \n4           7          5  [580910.0, 581498.0, 583054.0, 580285.0, 57964...   \n\n                                          gsr_series target  \n0  [192276.21875, 192022.203125, 191918.46875, 21...   A4V6  \n1  [177154.21875, 177107.75, 177348.90625, 177485...   A2V7  \n2  [174372.71875, 174527.484375, 174541.609375, 1...   A3V3  \n3  [159124.453125, 159066.09375, 159165.1875, 159...   A3V6  \n4  [153937.34375, 154182.421875, 154383.625, 1544...   A4V8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_id</th>\n      <th>series_id</th>\n      <th>ppg_series</th>\n      <th>gsr_series</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>1</td>\n      <td>[799777.0, 800031.0, 770530.0, 783136.0, 77184...</td>\n      <td>[192276.21875, 192022.203125, 191918.46875, 21...</td>\n      <td>A4V6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>2</td>\n      <td>[782666.0, 781125.0, 786560.0, 793689.0, 79145...</td>\n      <td>[177154.21875, 177107.75, 177348.90625, 177485...</td>\n      <td>A2V7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>3</td>\n      <td>[783222.0, 774507.0, 775848.0, 780017.0, 77640...</td>\n      <td>[174372.71875, 174527.484375, 174541.609375, 1...</td>\n      <td>A3V3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>4</td>\n      <td>[631551.0, 644157.0, 663125.0, 658480.0, 65069...</td>\n      <td>[159124.453125, 159066.09375, 159165.1875, 159...</td>\n      <td>A3V6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>5</td>\n      <td>[580910.0, 581498.0, 583054.0, 580285.0, 57964...</td>\n      <td>[153937.34375, 154182.421875, 154383.625, 1544...</td>\n      <td>A4V8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"features_target.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.360234Z","iopub.execute_input":"2024-11-09T15:59:21.360872Z","iopub.status.idle":"2024-11-09T15:59:21.368289Z","shell.execute_reply.started":"2024-11-09T15:59:21.360815Z","shell.execute_reply":"2024-11-09T15:59:21.367218Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(2190, 5)"},"metadata":{}}]},{"cell_type":"markdown","source":"# List all unique subject_id values","metadata":{}},{"cell_type":"code","source":"unique_subject_ids = features_target['subject_id'].unique()\nprint(unique_subject_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.369899Z","iopub.execute_input":"2024-11-09T15:59:21.370291Z","iopub.status.idle":"2024-11-09T15:59:21.380049Z","shell.execute_reply.started":"2024-11-09T15:59:21.370253Z","shell.execute_reply":"2024-11-09T15:59:21.378923Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"[ 7 47 19 22  2 35 50 23 10  5 61 36 20 45 60 64 41 39 32 25 42 52 75  8\n 38 12 55 49 62 53 70 34 18 79 65 67 78 28 66 56 72 26 74 15 69 77 43 71\n  1 58 59 30 14 76 57  9 46 21 44 40 80  6 11 68 63 37 51 33 54 48 29 24\n 73]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Count of unique subject_id values","metadata":{}},{"cell_type":"code","source":"unique_subject_count = features_target['subject_id'].nunique()\nprint(unique_subject_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.381311Z","iopub.execute_input":"2024-11-09T15:59:21.381727Z","iopub.status.idle":"2024-11-09T15:59:21.389358Z","shell.execute_reply.started":"2024-11-09T15:59:21.381672Z","shell.execute_reply":"2024-11-09T15:59:21.388343Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"73\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# After preprocessing check that each subject id has the same number of sequence ids (value counts)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib.ticker import MaxNLocator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom multiprocessing import cpu_count\n\nfrom sklearn.metrics import classification_report , confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.390682Z","iopub.execute_input":"2024-11-09T15:59:21.391032Z","iopub.status.idle":"2024-11-09T15:59:21.401923Z","shell.execute_reply.started":"2024-11-09T15:59:21.390995Z","shell.execute_reply":"2024-11-09T15:59:21.400810Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing to convert string values of surfaces to integers for neural network","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(features_target.target)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.403639Z","iopub.execute_input":"2024-11-09T15:59:21.404128Z","iopub.status.idle":"2024-11-09T15:59:21.414021Z","shell.execute_reply.started":"2024-11-09T15:59:21.404086Z","shell.execute_reply":"2024-11-09T15:59:21.412966Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"encoded_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.418556Z","iopub.execute_input":"2024-11-09T15:59:21.419068Z","iopub.status.idle":"2024-11-09T15:59:21.426733Z","shell.execute_reply.started":"2024-11-09T15:59:21.419027Z","shell.execute_reply":"2024-11-09T15:59:21.425681Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"array([29, 12, 17, ..., 21, 36,  2])"},"metadata":{}}]},{"cell_type":"code","source":"\n#labels are stored in the classes_ property in label encoder\nlabel_encoder.classes_\n# to reverse transformation if and when needed","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.428301Z","iopub.execute_input":"2024-11-09T15:59:21.428817Z","iopub.status.idle":"2024-11-09T15:59:21.440065Z","shell.execute_reply.started":"2024-11-09T15:59:21.428777Z","shell.execute_reply":"2024-11-09T15:59:21.438958Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"array(['A1V3', 'A1V4', 'A1V5', 'A1V6', 'A1V7', 'A1V8', 'A1V9', 'A2V2',\n       'A2V3', 'A2V4', 'A2V5', 'A2V6', 'A2V7', 'A2V8', 'A2V9', 'A3V1',\n       'A3V2', 'A3V3', 'A3V4', 'A3V5', 'A3V6', 'A3V7', 'A3V8', 'A3V9',\n       'A4V1', 'A4V2', 'A4V3', 'A4V4', 'A4V5', 'A4V6', 'A4V7', 'A4V8',\n       'A5V1', 'A5V2', 'A5V3', 'A5V4', 'A5V5', 'A5V6', 'A5V7', 'A5V8',\n       'A5V9', 'A6V1', 'A6V2', 'A6V3', 'A6V4', 'A6V5', 'A6V6', 'A6V7',\n       'A6V8', 'A7V1', 'A7V2', 'A7V3', 'A7V4', 'A7V5', 'A7V6', 'A7V7',\n       'A7V8', 'A8V1', 'A8V2', 'A8V3', 'A8V4', 'A8V5', 'A8V6', 'A8V7',\n       'A8V8', 'A8V9', 'A9V2', 'A9V3', 'A9V6', 'A9V7', 'A9V8', 'A9V9'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"features_target['label']=encoded_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.441814Z","iopub.execute_input":"2024-11-09T15:59:21.443004Z","iopub.status.idle":"2024-11-09T15:59:21.457836Z","shell.execute_reply.started":"2024-11-09T15:59:21.442960Z","shell.execute_reply":"2024-11-09T15:59:21.456788Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"unique_labels_count = features_target['label'].nunique()\nprint(unique_labels_count)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:13:13.373317Z","iopub.execute_input":"2024-11-09T16:13:13.374303Z","iopub.status.idle":"2024-11-09T16:13:13.380821Z","shell.execute_reply.started":"2024-11-09T16:13:13.374245Z","shell.execute_reply":"2024-11-09T16:13:13.379727Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"72\n","output_type":"stream"}]},{"cell_type":"code","source":"features_target.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:59:21.459218Z","iopub.execute_input":"2024-11-09T15:59:21.459700Z","iopub.status.idle":"2024-11-09T15:59:21.491113Z","shell.execute_reply.started":"2024-11-09T15:59:21.459647Z","shell.execute_reply":"2024-11-09T15:59:21.489867Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   subject_id  series_id                                         ppg_series  \\\n0           7          1  [799777.0, 800031.0, 770530.0, 783136.0, 77184...   \n1           7          2  [782666.0, 781125.0, 786560.0, 793689.0, 79145...   \n2           7          3  [783222.0, 774507.0, 775848.0, 780017.0, 77640...   \n3           7          4  [631551.0, 644157.0, 663125.0, 658480.0, 65069...   \n4           7          5  [580910.0, 581498.0, 583054.0, 580285.0, 57964...   \n\n                                          gsr_series target  label  \n0  [192276.21875, 192022.203125, 191918.46875, 21...   A4V6     29  \n1  [177154.21875, 177107.75, 177348.90625, 177485...   A2V7     12  \n2  [174372.71875, 174527.484375, 174541.609375, 1...   A3V3     17  \n3  [159124.453125, 159066.09375, 159165.1875, 159...   A3V6     20  \n4  [153937.34375, 154182.421875, 154383.625, 1544...   A4V8     31  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_id</th>\n      <th>series_id</th>\n      <th>ppg_series</th>\n      <th>gsr_series</th>\n      <th>target</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>1</td>\n      <td>[799777.0, 800031.0, 770530.0, 783136.0, 77184...</td>\n      <td>[192276.21875, 192022.203125, 191918.46875, 21...</td>\n      <td>A4V6</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>2</td>\n      <td>[782666.0, 781125.0, 786560.0, 793689.0, 79145...</td>\n      <td>[177154.21875, 177107.75, 177348.90625, 177485...</td>\n      <td>A2V7</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>3</td>\n      <td>[783222.0, 774507.0, 775848.0, 780017.0, 77640...</td>\n      <td>[174372.71875, 174527.484375, 174541.609375, 1...</td>\n      <td>A3V3</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>4</td>\n      <td>[631551.0, 644157.0, 663125.0, 658480.0, 65069...</td>\n      <td>[159124.453125, 159066.09375, 159165.1875, 159...</td>\n      <td>A3V6</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>5</td>\n      <td>[580910.0, 581498.0, 583054.0, 580285.0, 57964...</td>\n      <td>[153937.34375, 154182.421875, 154383.625, 1544...</td>\n      <td>A4V8</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Extract features and labels from features_target\nppg_series = features_target['ppg_series'].tolist()\ngsr_series = features_target['gsr_series'].tolist()\nlabels = features_target['label'].tolist()\n\n# Combine ppg_series and gsr_series as a dict for each sequence entry\nsequences = [{'ppg_series': ppg, 'gsr_series': gsr, 'label': label} for ppg, gsr, label in zip(ppg_series, gsr_series, labels)]\n\n# Split into train and test sets\ntrain_sequences, test_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:08:08.862793Z","iopub.execute_input":"2024-11-09T16:08:08.863235Z","iopub.status.idle":"2024-11-09T16:08:08.875179Z","shell.execute_reply.started":"2024-11-09T16:08:08.863197Z","shell.execute_reply":"2024-11-09T16:08:08.874103Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"len(train_sequences), len(test_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:08:10.363217Z","iopub.execute_input":"2024-11-09T16:08:10.363715Z","iopub.status.idle":"2024-11-09T16:08:10.371727Z","shell.execute_reply.started":"2024-11-09T16:08:10.363670Z","shell.execute_reply":"2024-11-09T16:08:10.370326Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(1752, 438)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchmetrics import Accuracy\nimport matplotlib.pyplot as plt\nfrom multiprocessing import cpu_count\n\n# Modified MixedEmotionDataset\nclass MixedEmotionDataset(Dataset):\n    \n    def __init__(self, sequences):\n        self.sequences = sequences\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        # Concatenate ppg_series and gsr_series, or pass separately based on the model's needs\n        ppg_series = torch.tensor(self.sequences[idx]['ppg_series'], dtype=torch.float32)\n        gsr_series = torch.tensor(self.sequences[idx]['gsr_series'], dtype=torch.float32)\n        label = torch.tensor(self.sequences[idx]['label'], dtype=torch.long)\n        \n        # Return combined sequence for the model\n        sequence = torch.cat((ppg_series, gsr_series))\n        \n        return {\n            'sequence': sequence,  # Combined input for the model\n            'label': label\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:08:12.578160Z","iopub.execute_input":"2024-11-09T16:08:12.578611Z","iopub.status.idle":"2024-11-09T16:08:15.631656Z","shell.execute_reply.started":"2024-11-09T16:08:12.578567Z","shell.execute_reply":"2024-11-09T16:08:15.630620Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# MixedEmotionDataLoader for train, validation, and test DataLoaders\nclass MixedEmotionDataLoader:\n    \n    def __init__(self, train_sequences, test_sequences, batch_size):\n        self.train_sequences = train_sequences\n        self.test_sequences = test_sequences\n        self.batch_size = batch_size\n        self.setup()\n        \n    def setup(self):\n        self.train_dataset = MixedEmotionDataset(self.train_sequences)\n        self.test_dataset = MixedEmotionDataset(self.test_sequences)\n        \n    def get_train_loader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=cpu_count()\n        )\n    \n    def get_val_loader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=cpu_count()\n        )\n    \n    def get_test_loader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=cpu_count()\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:08:30.408382Z","iopub.execute_input":"2024-11-09T16:08:30.409059Z","iopub.status.idle":"2024-11-09T16:08:30.419897Z","shell.execute_reply.started":"2024-11-09T16:08:30.409016Z","shell.execute_reply":"2024-11-09T16:08:30.418625Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class SequenceModel(nn.Module):\n    # classification, number of hidden units, number of layers for LSTM\n    \n    def __init__(self, n_features, n_classes , n_hidden=256, n_layers=3):\n        super().__init__()\n        \n        self.lstm = nn.LSTM(\n        input_size = n_features,\n        hidden_size=n_hidden,\n        num_layers=n_layers,\n        batch_first=True,\n        dropout=0.75)\n        \n        self.classifier = nn.Linear(n_hidden , n_classes)\n        \n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(x)\n        \n        out = hidden[-1]\n        \n        return self.classifier(out)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:06:04.489031Z","iopub.execute_input":"2024-11-09T16:06:04.489463Z","iopub.status.idle":"2024-11-09T16:06:04.497998Z","shell.execute_reply.started":"2024-11-09T16:06:04.489420Z","shell.execute_reply":"2024-11-09T16:06:04.496454Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Predictor Model\nclass MixedEmotionPredictor(nn.Module):\n    def __init__(self, n_features: int, n_classes: int):\n        super(MixedEmotionPredictor, self).__init__()\n        # Example sequence model (replace with your actual model)\n        self.model = SequenceModel(n_features, n_classes)  # Assuming SequenceModel outputs [batch_size, n_classes]\n        self.criterion = nn.CrossEntropyLoss()\n        self.accuracy_metric = Accuracy(task='multiclass', num_classes=n_classes)\n        \n        # Initialize lists to store metrics for each epoch\n        self.epoch_train_losses = []\n        self.epoch_val_losses = []\n        self.epoch_train_accuracies = []\n        self.epoch_val_accuracies = []\n        \n    def forward(self, x):\n        # Ensure the output is of shape [batch_size, n_classes]\n        return self.model(x)\n    \n    def compute_loss_and_accuracy(self, outputs, labels):\n        # Check if outputs have shape [batch_size, n_classes]\n        assert outputs.size(1) == len(labels.unique()), \"Mismatch in output size and label size\"\n        \n        # Cross-entropy expects the output to be [batch_size, n_classes] and labels to be [batch_size]\n        loss = self.criterion(outputs, labels)\n        predictions = torch.argmax(outputs, dim=1)  # Get predicted classes\n        accuracy = self.accuracy_metric(predictions, labels)\n        return loss, accuracy\n        \n    def plot_metrics(self):\n        num_epochs = min(len(self.epoch_train_losses), len(self.epoch_val_losses))\n        epochs = range(1, num_epochs + 1)\n        \n        plt.figure(figsize=(12, 5))\n        \n        # Plot Loss\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, self.epoch_train_losses, 'b', label='Training Loss')\n        plt.plot(epochs, self.epoch_val_losses, 'r', label='Validation Loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        # Plot Accuracy\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, self.epoch_train_accuracies, 'b', label='Training Accuracy')\n        plt.plot(epochs, self.epoch_val_accuracies, 'r', label='Validation Accuracy')\n        plt.title('Training and Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.show()\n        \ndef train(model, train_loader, val_loader, criterion, optimizer):\n    best_val_loss = float('inf')\n\n    for epoch in range(1, N_EPOCHS + 1):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        # Training step\n        for batch in train_loader:\n            sequences, labels = batch[\"ppg_series\"].to(device), batch[\"label\"].to(device)\n            optimizer.zero_grad()\n            outputs = model(sequences)\n            \n            # Ensure outputs have the shape [batch_size, n_classes]\n            assert outputs.size(1) == len(label_encoder.classes_), \"Mismatch in output size and label size\"\n            \n            loss, accuracy = model.compute_loss_and_accuracy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * sequences.size(0)\n            train_correct += (outputs.argmax(1) == labels).sum().item()\n            train_total += labels.size(0)\n        \n        avg_train_loss = train_loss / train_total\n        train_accuracy = train_correct / train_total\n        \n        # Validation step (same as training step)\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                sequences, labels = batch[\"ppg_series\"].to(device), batch[\"label\"].to(device)\n                outputs = model(sequences)\n                \n                assert outputs.size(1) == len(label_encoder.classes_), \"Mismatch in output size and label size\"\n                \n                loss, accuracy = model.compute_loss_and_accuracy(outputs, labels)\n                val_loss += loss.item() * sequences.size(0)\n                val_correct += (outputs.argmax(1) == labels).sum().item()\n                val_total += labels.size(0)\n                \n        avg_val_loss = val_loss / val_total\n        val_accuracy = val_correct / val_total\n\n        # Logging and saving model as before\n        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n        writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n\n        print(f\"Epoch {epoch}/{N_EPOCHS}, \"\n              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n              f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n        model.epoch_train_losses.append(avg_train_loss)\n        model.epoch_train_accuracies.append(train_accuracy)\n        model.epoch_val_losses.append(avg_val_loss)\n        model.epoch_val_accuracies.append(val_accuracy)\n        \n        best_val_loss = save_checkpoint(model, optimizer, epoch, avg_val_loss, best_val_loss)\n\n        # Plot training and validation metrics\n        self.plot_metrics()\n        \n    writer.close()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:26:08.409135Z","iopub.execute_input":"2024-11-09T16:26:08.409629Z","iopub.status.idle":"2024-11-09T16:26:08.439987Z","shell.execute_reply.started":"2024-11-09T16:26:08.409584Z","shell.execute_reply":"2024-11-09T16:26:08.438621Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\n\n# Parameters\nN_EPOCHS = 250\nBATCH_SIZE = 64\nCHECKPOINT_DIR = \"checkpoints\"\nLOG_DIR = \"lightning_logs/mixed_emotion\"\nBEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best-checkpoint.pth\")\n\n# Ensure directories exist\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Instantiate TensorBoard writer\nwriter = SummaryWriter(LOG_DIR)\n\n# Function to save the model checkpoint\ndef save_checkpoint(model, optimizer, epoch, val_loss, best_val_loss):\n    if val_loss < best_val_loss:\n        print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, BEST_MODEL_PATH)\n        return val_loss\n    return best_val_loss\n\n# Training loop\ndef train(model, train_loader, val_loader, criterion, optimizer):\n    best_val_loss = float('inf')\n\n    for epoch in range(1, N_EPOCHS + 1):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        # Training step\n        for batch in train_loader:\n            sequences = batch[\"sequence\"].to(device)  # Assuming batch[\"sequence\"] contains input data\n            labels = batch[\"label\"].to(device)  # Assuming batch[\"label\"] contains target labels\n            \n            optimizer.zero_grad()\n            outputs = model(sequences)\n            loss, accuracy = model.compute_loss_and_accuracy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * sequences.size(0)\n            train_correct += (outputs.argmax(1) == labels).sum().item()\n            train_total += labels.size(0)\n        \n        avg_train_loss = train_loss / train_total\n        train_accuracy = train_correct / train_total\n        \n        # Validation step\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                sequences = batch[\"sequence\"].to(device)\n                labels = batch[\"label\"].to(device)\n                outputs = model(sequences)\n                loss, accuracy = model.compute_loss_and_accuracy(outputs, labels)\n                val_loss += loss.item() * sequences.size(0)\n                val_correct += (outputs.argmax(1) == labels).sum().item()\n                val_total += labels.size(0)\n                \n        avg_val_loss = val_loss / val_total\n        val_accuracy = val_correct / val_total\n\n        # Logging to TensorBoard\n        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n        writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n\n        # Print metrics\n        print(f\"Epoch {epoch}/{N_EPOCHS}, \"\n              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n              f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n        \n        # Save metrics to model for plotting\n        model.epoch_train_losses.append(avg_train_loss)\n        model.epoch_train_accuracies.append(train_accuracy)\n        model.epoch_val_losses.append(avg_val_loss)\n        model.epoch_val_accuracies.append(val_accuracy)\n        \n        # Save checkpoint\n        best_val_loss = save_checkpoint(model, optimizer, epoch, avg_val_loss, best_val_loss)\n\n    writer.close()\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dynamically determine the number of features and classes from the data\n\n# # Get the number of features from one of the batches (assuming all batches have the same shape)\n# sample_batch = next(iter(train_loader))  # Get the first batch from train_loader\n# print(sample_batch)\n\n\n# # Assuming 'ppg_series' and 'gsr_series' are both in the batch, and both are tensors:\n# ppg_series = sample_batch[\"ppg_series\"]\n# gsr_series = sample_batch[\"gsr_series\"]\n\n# Determine the number of features by combining both ppg_series and gsr_series\n# Assuming ppg_series and gsr_series are tensors of shape [batch_size, num_features]\n# n_features = ppg_series.size(1) + gsr_series.size(1)  # Combine the features from both\n\n# n_features = len(features_target)\nn_features = 400\n# Determine the number of classes from label_encoder (assuming it's already fitted)\nn_classes = len(label_encoder.classes_)\n\n# Instantiate the model with dynamically determined n_features and n_classes\nmodel = MixedEmotionPredictor(n_features=n_features, n_classes=n_classes).to(device)\n# n_features = sample_batch[\"sequence\"].size(1)  # Assuming sequence is of shape [batch_size, n_features]\n# n_classes = len(label_encoder.classes_)  # Assuming `label_encoder` has already been fitted\n\n# # Instantiate the model without specifying the number of features or classes manually\n# model = MixedEmotionPredictor(n_features=n_features, n_classes=n_classes).to(device)\n\n# Instantiate optimizer and criterion\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Instantiate data loader\ndata_loader = MixedEmotionDataLoader(train_sequences, test_sequences, BATCH_SIZE)\ntrain_loader = data_loader.get_train_loader()\nval_loader = data_loader.get_val_loader()\n\n# Train the model\ntrain(model, train_loader, val_loader, criterion, optimizer)\n\n# Plot metrics\nmodel.plot_metrics()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T16:29:04.402970Z","iopub.execute_input":"2024-11-09T16:29:04.404400Z","iopub.status.idle":"2024-11-09T16:29:04.849969Z","shell.execute_reply.started":"2024-11-09T16:29:04.404333Z","shell.execute_reply":"2024-11-09T16:29:04.848120Z"},"trusted":true},"execution_count":81,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 140\u001b[0m\n\u001b[1;32m    137\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mget_val_loader()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Plot metrics\u001b[39;00m\n\u001b[1;32m    143\u001b[0m model\u001b[38;5;241m.\u001b[39mplot_metrics()\n","Cell \u001b[0;32mIn[81], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(sequences)\n\u001b[0;32m---> 48\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_and_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[77], line 22\u001b[0m, in \u001b[0;36mMixedEmotionPredictor.compute_loss_and_accuracy\u001b[0;34m(self, outputs, labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss_and_accuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, labels):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Check if outputs have shape [batch_size, n_classes]\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels\u001b[38;5;241m.\u001b[39munique()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in output size and label size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Cross-entropy expects the output to be [batch_size, n_classes] and labels to be [batch_size]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"],"ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-1, 0], but got 1)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}